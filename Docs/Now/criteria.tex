\documentclass[a4paper]{article}

\usepackage[utf8x]{inputenc}
\usepackage[russian, english]{babel}
\usepackage{cmap}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{geometry}
\usepackage{easyeqn}
\usepackage{bbm}
\usepackage{multirow}

\pagestyle{plain}

\title{Gaussian Process in Active Learning for Classification}
\author{Daria Kotova, Maxim Panov}

\begin{document}

\maketitle

\setlength{\marginparwidth}{1.5 cm}
\selectlanguage{english}

\begin{abstract}

There is a working file with description of what we are doing.

\end{abstract}

\section{Gaussian Process}

In this section we briefly review the definition of a Gaussian process and some important equations connected with it. A comprehensive overview of Gaussian processes is presented in \cite{gp}. \\
{\em  Gaussian process} $f\left(x\right)$ -- stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution. Here $x \in \mathbb{R}^D$, where $D$ - arbitrary natural number. \\
Gaussian process is defined by mean and covariance functions. Often mean function is chosen as constant 0. We introduce the notation:
\begin{align*}
m\left(x\right) & = \mathds{E}\left[ f\left(x\right) \right] = 0, \\
K\left(x, x'\right) & = \mathds{E} \left[ \left(f\left(x\right) - m\left(x\right)\right) \left(f\left(x'\right) - m\left(x'\right)\right)\right].
\end{align*}
\paragraph{Regression.} We want to get posterior on process $f\left(x\right)$, having observed points $X$. Prior is defined by:
\begin{EQA}[c]
\begin{pmatrix}
f \\
f_{*}
\end{pmatrix} \sim \mathcal{N}
\left(
0, 
\begin{pmatrix}
K(X, X) & K(X, X_{*}) \\
K(X_{*}, X) & K(X_{*}, X_{*})
\end{pmatrix}\right),
\end{EQA}
where $*$ corresponds to the test set. With Bayes formula and some calculations we show that posterior will look like:
\begin{EQA}[c]\label{aposterior}
\left( f_{*} | X, X_{*}, f \right) \sim \mathcal{N} (\hat{f}, \hat{\sigma}^2),
\end{EQA}
\begin{align*}
\text{where } \hat{f} & = K(X_{*}, X)K(X, X)^{-1}f \text{ is the posterior mean function,} \\
\hat{\sigma}^2 & = K(X_{*}, X_{*}) - K(X_{*},X)K(X, X)^{-1}K(X,X_{*})) \text{ is the posterior covariance function.}
\end{align*} \\
The reader can notice that interpretation of this result for covariance function is quite intuitive: $K(X_{*}, X_{*})$ is prior covariance and positive term, corresponding to new information, is substracted from prior knowledge.\\
To sum up, in regression case Gaussian process give us not only mean, but also variance -- the measure of uncertainty in a giving point. This property is advantage of Gaussian proccess. However, we should point out that matrix inversion in \eqref{aposterior} is extremely time consuming.

\section{Active Learning}

Now let's move on to basics of active learning approach. It is based on an assumption that model will give better results having less training points, if we allow it to choose points to train on by itself. \\
The algorithm is quite simple: the model chooses the next point which label it wants to get. Then ask an "oracle" for the label and somehow incorporates new knowledge. Then it repeats all the steps till some condition for stopping. Review of active learning can be found in \cite{generalav}. \\ 
We will consider pool-based sampling, which assumes that there is a small set of labeled data $\mathcal{L} = (x_1, ..., x_n)$ and a large pool of unlabeled data $\mathcal{U}$ available. New points are selectively drawn from the pool taking into account some acquisition function $g(u)$. Often new point $u^*$ delivers maximum of $g(u)$. This approach is also illustrated in Fig.\ref{activelearning}.\\
\begin{figure}[h]
\vspace{0 cm}
\center{\includegraphics[scale = 0.25]{definition1.png}}
\caption{Illustration of active learning algorithm using pool-based method.}
\label{activelearning}
\end{figure}
In this work we tried different $g(u)$. As a reference we used the work \cite{av} where general criterion and some specific variants were introduced. Now we will consider results of this work.\\
Let $f$ be the minimum norm function that interpolates labeles examples. Define $f_t^u(x)$ is the minimum norm interpolating function based on $\mathcal{L}$ and the point $u \in \mathcal{U}$. To get $f_t^u(x)$ we suppose that $u$ has label $t$ (since in supervised learning we have to have labeles for put points). Then our $g(u)$ can be presented as (both variants are possible):
\begin{EQA}[c]\label{score}
g(u) = \|f^u_t(v)\| \ or \ g(u) = \|f^u_t(v)-f(v)\|.
\end{EQA}

\section{Criteria}
Here we compare 6 different ways to choose new point $u^*$ from unlabeled data $\mathcal{U}$. Having assumed that $u^* = \underset{u \in \mathcal{U}}{argmax} ( g(u))$, we change $g(u)$ and compare results.
\begin{enumerate}
\item {\em Random   } -- just to check if results are adequate (rand).

\item {\em Variance } -- new point corresponds to the maximum of variance:
\begin{EQA}[c] 
g(u) = \hat{\sigma}^2(u) \text{\ (mvar).}
\end{EQA}

\item {\em 2-norm   } -- the criterion was introduced in \cite{av}. A new point is the argmax of: 
\begin{EQA}[c] 
g(u) = \|f^u_t(v)-f(v)\|_{\mathcal{U}} = \sqrt{\sum\limits_{v \in \mathcal{U}}(f^u_t(v)-f(v))^2} \text{\ (sqsm).}
\end{EQA}

\item {\em RKHS-norm} -- this criterion was also introduced in \cite{av}. However, we want to provide more comprehensive inference here: 
\begin{EQA}[c] 
g(u) = \|f^u_t(v)\|_{\mathcal{H}} = \vec{\widetilde{y}^T_t} \widetilde{K} \vec{\widetilde{y}_t} \text{\ computed with RKHS-norm (RKHS).}
\end{EQA}
Let's infer the $\|f^u_t(v)\|_{\mathcal{H}}$. \ Let $\widetilde{K} = 
\begin{pmatrix}
K			& \vec{a} \\
\vec{a}^T	& b
\end{pmatrix}
$, where $b = K(u, u), \vec{a} = [K(x_1, u), ... , K(x_n, u)]^T$.
Using Schur complement formula we have $\widetilde{K}^{-1}$ as:
\begin{EQA}
\widetilde{K}^{-1} & = &
\begin{pmatrix}
 K^{-1} + K^{-1}\vec{a}(b-\vec{a}^TK^{-1}\vec{a})^{-1}\vec{a}^TK^{-1} & 
-K^{-1}\vec{a}(b-\vec{a}^TK^{-1}\vec{a})^{-1} \\
-(b-\vec{a}^TK^{-1}\vec{a})^{-1}\vec{a}^TK^{-1} & (b-\vec{a}^TK^{-1}\vec{a})^{-1} \\
\end{pmatrix} 
\\ 
& = &\begin{pmatrix}
K^{-1} + \frac{K^{-1}\vec{a}\vec{a}^TK^{-1}}{b-\vec{a}^TK^{-1}\vec{a}} & 
\frac{-K^{-1}\vec{a}}{b-\vec{a}^TK^{-1}\vec{a}} \\
\frac{-\vec{a}^TK^{-1}}{b-\vec{a}^TK^{-1}\vec{a}} & \frac{1}{b-\vec{a}^TK^{-1}\vec{a}} \\
\end{pmatrix} .
\end{EQA}

Then $\|f^u_t(v)\|$ turns into:
\begin{EQA}
	\|f^u_t(v)\| & = & \vec{y}^TK^{-1}\vec{y} + \vec{y}^T \frac{K^{-1}\vec{aa}^TK^{-1}}{b - \vec{a}^TK^{-1}\vec{a}}\vec{y} - 2t\vec{y}^TK^{-1}\frac{\vec{a}}{b} -  2t\vec{y}^T \frac{K^{-1}\vec{aa}^TK^{-1}}{b - \vec{a}^TK^{-1}\vec{a}}\frac{\vec{a}}{b} + \frac{1}{b - \vec{a}^TK^{-1}\vec{a}} 
    \\
    & = & \vec{y}^TK^{-1}\vec{y} + \frac{\left(\vec{y}^TK^{-1}\vec{a}\right)^2}{b - \vec{a}^TK^{-1}\vec{a}} - 2t\frac{\vec{y}^TK^{-1}\vec{a}}{b}\left(1 + \frac{\vec{a}^TK^{-1}\vec{a}}{b - \vec{a}^TK^{-1}\vec{a}}\right) + \frac{t^2}{b - \vec{a}^TK^{-1}\vec{a}} 
    \\
    & = & \vec{y}^TK^{-1}\vec{y} + \frac{\left(\vec{y}^TK^{-1}\vec{a}\right)^2}{b - \vec{a}^TK^{-1}\vec{a}} - \frac{2t\left(\vec{y}K^{-1}\vec{a}\right)}{b - \vec{a}^TK^{-1}\vec{a}} + \frac{t^2}{b - \vec{a}^TK^{-1}\vec{a}} 
    \\
    & = & \vec{y}^TK^{-1}\vec{y}  + \frac{\left(\vec{y}^TK^{-1}\vec{a} - t\right)^2}{b - \vec{a}^TK^{-1}\vec{a}} = \vec{y}^TK^{-1}\vec{y}  + \frac{\left(f(u) - y(u)\right)^2}{b - \vec{a}^TK^{-1}\vec{a}}.
\end{EQA}
That means 
\begin{EQA}[c]
g(u) = \|f^u_t(v)\|_{\mathcal{H}} = \vec{\widetilde{y}^T_t} \widetilde{K} \vec{\widetilde{y}_t} = \|f(v)\|_{\mathcal{H}} + \frac{(1 - t \cdot f(u))^2}{b - \vec{a}^T K^{-1} \vec{a}},
\end{EQA}
It is important to emphasize that denominator here is the posterior variance of the model at the point $u$ (see \eqref{aposterior}).

\item {\em RKHS-norm $\cdot$ variance} -- RKHS-norm turns out to have too huge values. Multiplying by variance allows solve the problem and also make quality of the criterion better. This idea was introduced in \cite{Hvar} (Hvar). 
\begin{EQA}[c]
g(u) = (\|f^u_t(v)\| - \|f(v)\|_{\mathcal{H}}) \cdot \hat{\sigma}^2(u) = (1 - t \cdot f(u))^2.
\end{EQA}
\item {\em 2-norm in formula} -- in sqsm we directly learned new model for each $u$. However, it can be simplified using previous knowledge (l2fm): \\
Let $\widetilde{a} = \begin{pmatrix}
\vec{a} \\
K(u,v)
\end{pmatrix} $ . Where $u$ is the new point and $v \in \mathcal{U}$ is the point we calculate $f(v)$ for. \\
Let's compute difference $f^u_t(v) - f(v)$:\\
\begin{EQA}
	f^u_t(v) - f(v) & = & 	\vec{\widetilde{a}}^T\widetilde{K}^{-1}\vec{\widetilde{y}} - \vec{a}^TK^{-1}\vec{y} 
	\\ 
	& = & \begin{pmatrix}
\vec{a}^T & K(u,v)
\end{pmatrix}
\begin{pmatrix}
K^{-1} + \frac{K^{-1}\vec{a}\vec{a}^TK^{-1}}{b-\vec{a}^TK^{-1}\vec{a}} & 
\frac{-K^{-1}\vec{a}}{b-\vec{a}^TK^{-1}\vec{a}} \\
\frac{-\vec{a}^TK^{-1}}{b-\vec{a}^TK^{-1}\vec{a}} & \frac{1}{b-\vec{a}^TK^{-1}\vec{a}} \\
\end{pmatrix}
\begin{pmatrix}
\vec{y} \\
t
\end{pmatrix} - \vec{a}^TK^{-1}\vec{y} 
	\\ 
	& = & \frac{(\vec{a}^TK^{-1}\vec{a} - K(u,v))(\vec{a}^TK^{-1}\vec{y} - t)}{b - \vec{a}^TK^{-1}\vec{a}}.\\	
\end{EQA}
Finally, we get:
\begin{EQA}
g(u) & = & \|f^u_t(v)-f(v)\|_{\mathcal{U}} = \sqrt{\sum\limits_{v \in \mathcal{U}}(f^u_t(v)-f(v))^2} \\
	& = & \sqrt{\sum\limits_{v \in \mathcal{U}}\left( 
	\frac{(\vec{a}^TK^{-1}\vec{a} - K(u,v))(\vec{a}^TK^{-1}\vec{y} - t)}{b - \vec{a}^TK^{-1}\vec{a}}
	\right)^2}.
\end{EQA}
\end{enumerate}

\section{Experiments}

\subsection{2 blobs}

We took 2 blobs of 2-dimensional points that consist of 1000 points total. 500 of them went to the test set and training size was changing from 20 to 500 points. Figures \ref{recent1}, \ref{recent2}, \ref{recent3} show what points different score-functions tend to choose for cases when in the training set are 50, 100 and 150 points.

\begin{figure}[h]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{50recent_points_rand.png}} Random
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{50recent_points_mvar.png}} Max-variance
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{50recent_points_l2fm.png}} 2-norm in formula
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{50recent_points_RKHS.png}} RKHS
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{50recent_points_Hvar.png}} RKHS $\cdot$ variance
\end{minipage}
\caption{Blue points - unlabeled data given to the model to choose next point from. Red points - 10 points that were chosen. Training set contains 50 points.}
\label{recent1}
\end{figure}

\begin{figure}[h]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100recent_points_rand.png}} Random
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100recent_points_mvar.png}} Max-variance
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100recent_points_l2fm.png}} 2-norm in formula
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100recent_points_RKHS.png}} RKHS
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100recent_points_Hvar.png}} RKHS $\cdot$ variance
\end{minipage}
\caption{Blue points - unlabeled data given to the model to choose next point from. Red points - 10 points that were chosen. Training set contains 100 points.}
\label{recent2}
\end{figure}

\begin{figure}[h]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{150recent_points_rand.png}} Random
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{150recent_points_mvar.png}} Max-variance
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{150recent_points_l2fm.png}} 2-norm in formula
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{150recent_points_RKHS.png}} RKHS
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{150recent_points_Hvar.png}} RKHS $\cdot$ variance
\end{minipage}
\caption{Blue points - unlabeled data given to the model to choose next point from. Red points - 10 points that were chosen. Training set contains 150 points.}
\label{recent3}
\end{figure}

\clearpage
Next we took the same dataset and plot 3-d surfaces of score-functions and also tried to project them onto a plane using contour plots. The results are presented on figures \ref{surface_plots} and \ref{contour_plots}. Training dataset is containing 100 points.

\begin{figure}[h]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100surface_mvar.png}} Max-variance
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100surface_l2fm.png}} 2-norm in formula
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100surface_RKHS.png}} RKHS
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100surface_Hvar.png}} RKHS $\cdot$ variance
\end{minipage}

\caption{Blue points - training dataset. Surface is drawn on the grid 100x100 points in total. The logarithmic scale is selected along the third axis.}
\label{surface_plots}
\end{figure}

\begin{figure}[h]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100contour_mvar.png}} Max-variance
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100contour_l2fm.png}} 2-norm in formula
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100contour_RKHS.png}} RKHS
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale = 0.5]{100contour_Hvar.png}} RKHS $\cdot$ variance
\end{minipage}

\caption{Blue points - training examples from the first class. Red points - from the second class. Black stars - recently chosen points. Contour is drawn on the grid 100x100 points in total.}
\label{contour_plots}
\end{figure}

\clearpage
On the figure \ref{TwoDimacc} accuracies depending on the size of training dataset and method are shown.

\begin{figure}[h]
\begin{minipage}[c]{0.49\linewidth}
\center{\includegraphics[scale = 0.4]{accuracy2dim2.png}} Size of train datset changes from 50 to 250 points. Traditional methods included.
\end{minipage}
\vfill
\begin{minipage}[c]{0.49\linewidth}
\center{\includegraphics[scale = 0.4]{accuracy2dimGP2.png}} Size of train datset changes from 50 to 250 points.
\end{minipage}
\caption{Two 2-dimensional blobs of points.}
\label{TwoDimacc}
\end{figure}

\clearpage
\subsection{Skin}

Skin dataset  is collected by randomly sampling B,G,R values from face images of various age groups (young, middle, and old), race groups (white, black, and asian), and genders obtained from FERET database and PAL database. Total learning sample size is 245057; out of which 50859 is the skin samples and 194198 is non-skin samples. 
\\
We used 2000 random samples from the dataset. 1000 went to the test dataset and training dataset changed from 10 to 1000 points. We used it to compare not only different score-functions, but also compare active-learning approach with some traditional methods: logistic regression, k nearest neighbours, decision tree and random forest.
\\
On the figure \ref{Skinacc} you can see how accuracy of the certain method depends on the size of training dataset. Traditional methods shared train dataset with random score.

\begin{figure}[h]
\begin{minipage}[c]{0.49\linewidth}
\center{\includegraphics[scale = 0.4]{accuracyMethodsSkin4.png}} Size of train datset changes from 10 to 250 points.
\end{minipage}
\vfill
\begin{minipage}[c]{0.49\linewidth}
\center{\includegraphics[scale = 0.4]{accuracySkin1.png}} Size of train datset changes from 10 to 1000 points.
\end{minipage}
\caption{Compartion of different approaches to classification task on Skin dataset.}
\label{Skinacc}
\end{figure}

\clearpage
\subsection{HTRU2}

HTRU2 is a data set which describes a sample of pulsar candidates collected during the High Time Resolution Universe Survey (South).
\\

Here the legitimate pulsar examples are a minority positive class, and spurious examples the majority negative class. At present multi-class labels are unavailable, given the costs associated with data annotation.
\\
The data set shared here contains 16,259 spurious examples caused by RFI/noise, and 1,639 real pulsar examples. These examples have all been checked by human annotators. 
\\ 
We used total 1000 points from the dataset, test dataset containd 500 points, and the training dataset changed from 10 to 500 points.

\begin{figure}[h]
\begin{minipage}[c]{0.49\linewidth}
\center{\includegraphics[scale = 0.4]{accuracyHTRUGP1.png}} Size of train datset changes from 10 to 500 points.
\end{minipage}
\caption{HTRU dataset.}
\label{HTRU}
\end{figure}

\subsection{Time comparsion}

In the Table \ref{time} there are times in seconds that it take to calculate score-function during the experiment.

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
 \hline
 \cline{5-9}
& & & & \multicolumn{5}{ c| }{Score Times} \\ \cline{5-9}
 \hline
 Dataset Name & Dimensionality & Start train size & End train size &
 rand & mvar & RKHS & Hvar & l2fm \\
 \hline
 TwoDim & 2 & 50 & 500  & 0.016 & 0.48  & 0.96  & 0.88  & 1.75  \\
 Skin   & 3 & 50 & 500  & 0.012 & 0.57  & 1.16  & 1.07  & 2.17  \\
 HTRU2  & 8 & 50 & 500  & 0.013 & 0.57  & 1.16  & 1.06  & 2.17  \\
 \hline
 TwoDim & 2 & 20 & 1000 & 0.034 & 2.82  & 6.13  & 5.20  & 12.99 \\
 Skin   & 3 & 20 & 1000 & 0.039 & 6.27  & 15.71 & 11.87 & 40.57 \\
 \hline
\end{tabular}
\caption{ Comparsion of times needed to calculate different score functions.}
\label{time}
\end{table}

\subsection{Conclusions}
From the experiments we certainly can say that choosing new point in active learning process by the maximum of variance leads to worse quality. Other methods perfom better or at least as well as random sampling.

\clearpage
\begin{thebibliography}{}

\bibitem[1]{Hvar}
Burnaev E., Panov M. (2015) 
Adaptive Design of Experiments Based on Gaussian Processes. In: Gammerman A., Vovk V., Papadopoulos H. (eds) Statistical Learning and Data Sciences. SLDS 2015. 
Lecture Notes in Computer Science, vol 9047. Springer, Cham

\bibitem[2]{gp}
C. E. Rasmussen, C. K. I. Williams, 
Gaussian Processes for Machine Learning, 
the MIT Press, 2006, ISBN 026218253X

\bibitem[3]{av}
Mina Karzand, Robert D. Nowak:
Active Learning in the Overparameterized
and Interpolating Regime
arXiv preprint arXiv:1905.12782,2019

\bibitem[4]{generalav}
Burr Settles. 
Active Learning Literature Survey.  
2010.

\end{thebibliography}

\end{document}
